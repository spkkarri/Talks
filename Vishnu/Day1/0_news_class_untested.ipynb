{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple PyTorch Text Classification (AG News)\n",
    "\n",
    "This notebook demonstrates a basic text classification pipeline using PyTorch, Transformers tokenizers, and the Datasets library. It aims to be runnable within ~10 minutes on Google Colab using a simple custom model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install necessary libraries if they are not already available in the Colab environment.\n",
    "- `datasets`: For loading datasets easily.\n",
    "- `transformers`: For tokenizers (and potentially pre-trained models, though we use a simple one here).\n",
    "- `scikit-learn`: For evaluation metrics.\n",
    "- `accelerate`: Often helpful for optimizing PyTorch training, especially with Transformers.\n",
    "- `pandas`, `seaborn`, `matplotlib`: For data handling and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Installation ---\n",
    "print(\"Installing required packages...\")\n",
    "!pip install torch datasets transformers scikit-learn accelerate pandas seaborn matplotlib -q\n",
    "print(\"Installation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "Import libraries needed for the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup and Configuration\n",
    "\n",
    "Set up the environment (device), define key hyperparameters and configuration parameters, and start a timer to track execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Setup and Configuration ---\n",
    "# Start timer to track execution time.\n",
    "start_time = time.time()\n",
    "\n",
    "# Set device to GPU (cuda) if available, otherwise use CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration parameters\n",
    "MODEL_NAME = \"bert-base-uncased\" # Using tokenizer from this model\n",
    "DATASET_NAME = \"ag_news\"\n",
    "MAX_LENGTH = 128 # Max sequence length for tokenizer\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 100 # Dimension for our simple embedding layer\n",
    "LEARNING_RATE = 5e-4 # Learning rate for the optimizer\n",
    "EPOCHS = 3 # Number of training epochs (keep low for speed)\n",
    "TRAIN_SUBSET_SIZE = 10000 # Use a subset for faster training\n",
    "TEST_SUBSET_SIZE = 1000 # Use a subset for faster evaluation\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "Load the AG News dataset (a standard text classification benchmark with 4 classes: World, Sports, Business, Sci/Tech) using the `datasets` library. We load only subsets of the train and test splits for faster execution in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Load Dataset ---\n",
    "print(f\"Loading dataset '{DATASET_NAME}'...\")\n",
    "dataset = load_dataset(DATASET_NAME, split={\n",
    "    'train': f'train[:{TRAIN_SUBSET_SIZE}]',\n",
    "    'test': f'test[:{TEST_SUBSET_SIZE}]'\n",
    "})\n",
    "\n",
    "# AG News class labels: 0: World, 1: Sports, 2: Business, 3: Sci/Tech\n",
    "# Create a mapping for easy reference\n",
    "label_map = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "num_classes = len(dataset['train'].unique(\"label\"))\n",
    "print(f\"Dataset loaded. Number of classes: {num_classes}\")\n",
    "print(f\"Class labels: {label_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore the Data\n",
    "\n",
    "Let's look at a few raw examples from the training set to understand the text and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Explore the Data ---\n",
    "print(\"\\n--- Sample Training Data ---\")\n",
    "# Display first 5 samples using Pandas DataFrame for better formatting\n",
    "df_samples = pd.DataFrame(dataset['train'][:5])\n",
    "df_samples['label_name'] = df_samples['label'].map(label_map)\n",
    "print(df_samples[['text', 'label', 'label_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Tokenizer\n",
    "\n",
    "Load a tokenizer from the `transformers` library. We use the tokenizer associated with `bert-base-uncased`. The tokenizer converts raw text into numerical representations (token IDs) that the model can understand. It also handles tasks like splitting words into subwords if necessary (though less relevant for our simple embedding model here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Load Tokenizer ---\n",
    "print(f\"\\nLoading tokenizer '{MODEL_NAME}'...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "VOCAB_SIZE = tokenizer.vocab_size # Get the size of the tokenizer's vocabulary\n",
    "print(f\"Tokenizer loaded. Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Padding token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preprocessing and Tokenization Visualization\n",
    "\n",
    "Define a function to apply the tokenizer to our text data. This function will:\n",
    "1.  Tokenize the text.\n",
    "2.  Pad sequences to `MAX_LENGTH` so they all have the same size.\n",
    "3.  Truncate sequences longer than `MAX_LENGTH`.\n",
    "\n",
    "We then apply this function to the entire dataset using `.map()` for efficiency and set the format to PyTorch tensors. Finally, we visualize the tokenization output for one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Preprocessing ---\n",
    "# Define a function to tokenize the text data.\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Apply the preprocessing function to the dataset.\n",
    "print(\"\\nPreprocessing dataset (tokenizing)...\")\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set the format to PyTorch tensors.\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_dataset = encoded_dataset['train']\n",
    "test_dataset = encoded_dataset['test']\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- Tokenization Visualization ---\n",
    "print(\"\\n--- Sample Tokenized Data ---\")\n",
    "sample_processed = train_dataset[0]\n",
    "print(f\"Original Text: {dataset['train'][0]['text'][:100]}...\")\n",
    "print(f\"Input IDs (sample): {sample_processed['input_ids'][:20]}...\") # Show first 20 IDs\n",
    "print(f\"Attention Mask (sample): {sample_processed['attention_mask'][:20]}...\") # 1 for real tokens, 0 for padding\n",
    "# Decode the IDs back to tokens to see the result\n",
    "tokens = tokenizer.convert_ids_to_tokens(sample_processed['input_ids'][:20])\n",
    "print(f\"Tokens (sample): {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders\n",
    "\n",
    "Create PyTorch `DataLoader` objects. These efficiently load data in batches during training and evaluation, and can automatically shuffle the training data each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Create DataLoaders ---\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "print(f\"DataLoaders created with batch size {BATCH_SIZE}.\")\n",
    "print(f\"Number of batches in train_dataloader: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in test_dataloader: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define the Model\n",
    "\n",
    "Define a simple text classification model using PyTorch's `nn.Module`.\n",
    "1.  `nn.Embedding`: Converts input token IDs into dense vector representations (embeddings). `padding_idx` ensures padding tokens don't affect learning.\n",
    "2.  Mean Pooling: Averages the embeddings of all non-padding tokens in a sequence to get a single vector representation for the entire sequence.\n",
    "3.  `nn.Linear`: A fully connected layer that takes the pooled sequence representation and outputs scores for each of the possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Define the Model ---\n",
    "class SimpleTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        # 1. Get Embeddings\n",
    "        embedded = self.embedding(input_ids)\n",
    "\n",
    "        # 2. Apply Masking and Mean Pooling\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(embedded.size()).float()\n",
    "        embedded = embedded * mask_expanded # Zero out padding embeddings\n",
    "        sum_embeddings = torch.sum(embedded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9) # Count non-padding tokens\n",
    "        mean_embeddings = sum_embeddings / sum_mask # Calculate mean\n",
    "\n",
    "        # 3. Pass through Linear Layer\n",
    "        return self.fc(mean_embeddings)\n",
    "\n",
    "print(\"\\nModel definition complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Instantiate Model, Loss, and Optimizer\n",
    "\n",
    "Create an instance of our `SimpleTextClassifier` model, define the loss function (`CrossEntropyLoss` for multi-class classification), and choose an optimizer (`Adam`). Move the model to the appropriate device (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. Instantiate Model, Loss, and Optimizer ---\n",
    "print(\"\\nInstantiating model, loss function, and optimizer...\")\n",
    "model = SimpleTextClassifier(VOCAB_SIZE, EMBED_DIM, num_classes).to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Model, Loss, Optimizer instantiated.\")\n",
    "\n",
    "# Print model structure and parameter count\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Loop\n",
    "\n",
    "Train the model for the specified number of epochs. In each epoch:\n",
    "1.  Iterate through batches provided by the `train_dataloader`.\n",
    "2.  Move the batch data to the device.\n",
    "3.  Clear previous gradients (`optimizer.zero_grad()`).\n",
    "4.  Perform a forward pass to get model predictions.\n",
    "5.  Calculate the loss between predictions and true labels.\n",
    "6.  Perform a backward pass to compute gradients (`loss.backward()`).\n",
    "7.  Update the model weights using the optimizer (`optimizer.step()`).\n",
    "8.  Track and report the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 11. Training Loop ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "model.train() # Set the model to training mode\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "             print(f\"  Batch {i+1}/{len(train_dataloader)}, Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_end_time - epoch_start_time:.2f} seconds.\")\n",
    "    print(f\"Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluation\n",
    "\n",
    "Evaluate the trained model on the test dataset.\n",
    "1.  Set the model to evaluation mode (`model.eval()`).\n",
    "2.  Iterate through the `test_dataloader`.\n",
    "3.  Disable gradient calculations (`torch.no_grad()`) for efficiency.\n",
    "4.  Get model predictions for each batch.\n",
    "5.  Collect all predictions and true labels.\n",
    "6.  Calculate and display evaluation metrics (accuracy, classification report, confusion matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 12. Evaluation ---\n",
    "print(\"\\n--- Starting Evaluation ---\")\n",
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(batch)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=label_map.values(), digits=4)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_map.values(), yticklabels=label_map.values())\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Evaluation Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Timings\n",
    "\n",
    "Calculate and display the total execution time for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 13. Final Timings ---\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nTotal execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Optional: Example Prediction\n",
    "\n",
    "Demonstrate how to use the trained model to classify a single, new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: Example Prediction ---\n",
    "print(\"\\n--- Example Prediction ---\")\n",
    "text = \"The government announced new economic policies today.\" # Example sentence\n",
    "model.eval() # Ensure model is in evaluation mode\n",
    "with torch.no_grad(): # No need to track gradients for prediction\n",
    "    # 1. Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "    \n",
    "    # 2. Move inputs to the correct device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 3. Create the batch structure expected by the model\n",
    "    batch = {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']}\n",
    "    \n",
    "    # 4. Get model output (scores)\n",
    "    output = model(batch)\n",
    "    \n",
    "    # 5. Get predicted class index\n",
    "    prediction_idx = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # 6. Map index to label name\n",
    "    predicted_label = label_map.get(prediction_idx, 'Unknown')\n",
    "\n",
    "    print(f\"Sentence: '{text}'\")\n",
    "    print(f\"Predicted class index: {prediction_idx}\")\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "print(\"\\n--- End of Notebook ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
