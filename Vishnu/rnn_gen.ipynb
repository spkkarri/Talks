{"cells":[{"cell_type":"markdown","metadata":{"id":"i8_0DqbajR-C"},"source":["# Simple RNN for Character-Level Text Generation\n","\n","**Inspired by A. Karpathy's \"The Unreasonable Effectiveness of Recurrent Neural Networks\"**\n","\n","**Goal:** To build and understand a *minimal* character-level Recurrent Neural Network (RNN) using PyTorch, similar to the basic examples discussed by Andrej Karpathy. We aim to predict the next character in a sequence and generate some text.\n","\n","**Focus:** Understanding the core RNN mechanism (hidden state as memory), character embeddings, and the generation process via sampling.\n","\n","**Disclaimer:** We are intentionally using a *simple `nn.RNN`* (not LSTM/GRU) and a very small dataset/short training time (~5 mins) for illustrative purposes. As Karpathy notes, simple RNNs struggle with long-term dependencies. **Do not expect high-quality generated text.** Expect repetition and nonsensical sequences. The goal is to see the *potential* and understand the mechanics."]},{"cell_type":"markdown","metadata":{"id":"WnOYwMcojR-F"},"source":["## 1. Setup: Imports and Configuration"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqCwBziSjR-G","executionInfo":{"status":"ok","timestamp":1746349744841,"user_tz":-330,"elapsed":4518,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"f5ec5c8a-a3d9-4875-ff65-b23a899ff109"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Using simple RNN with 2 layers, HIDDEN_DIM=256\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","import time\n","import math\n","import random\n","\n","# Configuration\n","SEQ_LENGTH = 40      # How many steps to unroll the RNN for backpropagation\n","EMBEDDING_DIM = 64   # Dimension of character embeddings\n","HIDDEN_DIM = 256     # Size of the RNN's hidden state 'memory'\n","NUM_LAYERS = 2       # Number of stacked RNN layers (1 or 2 is typical for simple examples)\n","BATCH_SIZE = 64      # Number of sequences per batch\n","LEARNING_RATE = 0.003 # Learning rate\n","EPOCHS = 15          # Number of training epochs (keep low for speed)\n","GENERATION_LENGTH = 200 # How many characters to generate\n","GRAD_CLIP = 1.0      # Gradient clipping value to prevent exploding gradients\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# For reproducibility\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","\n","print(f\"Using simple RNN with {NUM_LAYERS} layers, HIDDEN_DIM={HIDDEN_DIM}\")"]},{"cell_type":"markdown","metadata":{"id":"lat08ob9jR-H"},"source":["## 2. Data Preparation\n","\n","We need to convert our text into numbers that the RNN can process.\n","\n","1.  **Corpus:** A small piece of text.\n","2.  **Vocabulary:** The set of unique characters in the text.\n","3.  **Mappings:** Dictionaries to convert characters to integers (`char_to_int`) and back (`int_to_char`).\n","4.  **Sequences:** Create input sequences of length `SEQ_LENGTH` and corresponding target characters (the character immediately following each sequence)."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owrSU684jR-H","executionInfo":{"status":"ok","timestamp":1746349744876,"user_tz":-330,"elapsed":40,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"17bbf20f-43d9-447f-85f5-24d276ce326c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Corpus length: 798 characters\n","Vocabulary size: 31 unique characters\n","Vocabulary: \n"," (),.abcdefghiklmnopqrstuvwxyz\n","\n","Number of sequences created: 758\n","Sample Input : 'f linguistics, computer science, and art'\n","Sample Target: 'i'\n","\n","Created DataLoader with 11 batches.\n"]}],"source":["# 1. Define Text Corpus (Small snippet for speed)\n","text = \"\"\"\n","Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n","The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them.\n","The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n","Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n","Recurrent neural networks (RNNs) were once commonly used for such tasks.\n","\"\"\".lower() # Use lowercase\n","\n","# 2. Create Vocabulary\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","\n","# 3. Create Mappings\n","char_to_int = {ch: i for i, ch in enumerate(chars)}\n","int_to_char = {i: ch for i, ch in enumerate(chars)}\n","\n","print(f\"Corpus length: {len(text)} characters\")\n","print(f\"Vocabulary size: {vocab_size} unique characters\")\n","print(f\"Vocabulary: {''.join(chars)}\")\n","\n","# 4. Generate Sequences and Targets\n","input_seqs_int = []\n","target_chars_int = []\n","for i in range(len(text) - SEQ_LENGTH):\n","    seq_in = text[i:i + SEQ_LENGTH]\n","    seq_out = text[i + SEQ_LENGTH]\n","    input_seqs_int.append([char_to_int[ch] for ch in seq_in])\n","    target_chars_int.append(char_to_int[seq_out])\n","\n","num_sequences = len(input_seqs_int)\n","print(f\"\\nNumber of sequences created: {num_sequences}\")\n","\n","# Display a sample\n","sample_idx = 50\n","print(f\"Sample Input : '{''.join([int_to_char[i] for i in input_seqs_int[sample_idx]])}'\")\n","print(f\"Sample Target: '{int_to_char[target_chars_int[sample_idx]]}'\")\n","\n","# 5. Create PyTorch Dataset and DataLoader\n","X = torch.tensor(input_seqs_int, dtype=torch.long)\n","y = torch.tensor(target_chars_int, dtype=torch.long)\n","\n","class CharDataset(Dataset):\n","    def __init__(self, sequences, targets):\n","        self.sequences = sequences\n","        self.targets = targets\n","    def __len__(self):\n","        return len(self.sequences)\n","    def __getitem__(self, idx):\n","        return self.sequences[idx], self.targets[idx]\n","\n","dataset = CharDataset(X, y)\n","dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","print(f\"\\nCreated DataLoader with {len(dataloader)} batches.\")"]},{"cell_type":"markdown","metadata":{"id":"KCc5vbqZjR-I"},"source":["## 3. Define the Simple RNN Model\n","\n","This follows the standard structure:\n","1.  `nn.Embedding`: Learns a vector for each character.\n","2.  `nn.RNN`: The core recurrent layer. Takes the current character's embedding and the previous hidden state, outputs a new hidden state and an output vector.\n","3.  `nn.Linear`: Maps the RNN's output for the last character in the sequence to scores for *every* character in the vocabulary (predicting the next one)."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QiuWgRJjR-J","executionInfo":{"status":"ok","timestamp":1746349744897,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"672e3665-2cf1-4991-c076-8840e98512f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model Definition (Simple RNN):\n","CharRNN(\n","  (embedding): Embedding(31, 64)\n","  (rnn): RNN(64, 256, num_layers=2, batch_first=True)\n","  (fc): Linear(in_features=256, out_features=31, bias=True)\n",")\n","\n","Total trainable parameters: 223,967\n"]}],"source":["class CharRNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        # Use nn.RNN here!\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x, h):\n","        # x shape: (batch_size, seq_length)\n","        # h shape: (num_layers, batch_size, hidden_dim)\n","        embedded = self.embedding(x) # -> (batch_size, seq_length, embedding_dim)\n","        # Pass embedded sequence and hidden state through RNN\n","        out, h_out = self.rnn(embedded, h)\n","        # out shape: (batch_size, seq_length, hidden_dim) - Output from last RNN layer for each time step\n","        # h_out shape: (num_layers, batch_size, hidden_dim) - Final hidden state for all layers\n","\n","        # We take the RNN output from the *very last* time step\n","        last_time_step_out = out[:, -1, :] # -> (batch_size, hidden_dim)\n","\n","        # Pass this through the fully connected layer to get scores for next char\n","        scores = self.fc(last_time_step_out) # -> (batch_size, vocab_size)\n","        return scores, h_out\n","\n","    def init_hidden(self, batch_size):\n","        # Initial hidden state (usually zeros)\n","        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n","\n","# Instantiate the model\n","model = CharRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n","print(\"Model Definition (Simple RNN):\")\n","print(model)\n","total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"\\nTotal trainable parameters: {total_params:,}\")"]},{"cell_type":"markdown","metadata":{"id":"hKjGt82RjR-K"},"source":["## 4. Training the RNN\n","\n","The training loop involves iterating through the data, feeding sequences to the model, calculating the loss (how wrong the predictions are), and updating the model's weights.\n","\n","**Key RNN aspects:**\n","*   **Hidden State:** The hidden state `h` is initialized at the start and passed through the RNN along with the input. The RNN outputs an updated hidden state, which is then used for the *next* batch (after detaching).\n","*   **Detaching:** `h = h.detach()` prevents gradients from flowing back endlessly through the entire training history, which is computationally infeasible and usually not helpful. We only backpropagate through the current sequence (`SEQ_LENGTH`).\n","*   **Gradient Clipping:** `clip_grad_norm_` helps prevent the exploding gradient problem, where gradients become excessively large and destabilize training."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKiULqXqjR-L","executionInfo":{"status":"ok","timestamp":1746349765568,"user_tz":-330,"elapsed":20670,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"d2cf828f-3c3e-42ec-fcf5-be8a90cceda7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Starting Training (Simple RNN) ---\n","Epoch 1/15 | Avg Loss: 2.9817 | Time: 0.95s\n","Epoch 2/15 | Avg Loss: 2.3214 | Time: 0.64s\n","Epoch 3/15 | Avg Loss: 2.0089 | Time: 0.66s\n","Epoch 4/15 | Avg Loss: 1.7189 | Time: 0.64s\n","Epoch 5/15 | Avg Loss: 1.4153 | Time: 0.64s\n","Epoch 6/15 | Avg Loss: 1.1765 | Time: 0.62s\n","Epoch 7/15 | Avg Loss: 0.9544 | Time: 0.63s\n","Epoch 8/15 | Avg Loss: 0.7394 | Time: 0.63s\n","Epoch 9/15 | Avg Loss: 0.5727 | Time: 0.63s\n","Epoch 10/15 | Avg Loss: 0.3940 | Time: 0.65s\n","Epoch 11/15 | Avg Loss: 0.3020 | Time: 0.64s\n","Epoch 12/15 | Avg Loss: 0.2283 | Time: 0.64s\n","Epoch 13/15 | Avg Loss: 0.1880 | Time: 0.65s\n","Epoch 14/15 | Avg Loss: 0.1434 | Time: 0.64s\n","Epoch 15/15 | Avg Loss: 0.1143 | Time: 0.64s\n","--- Training Finished in 9.90 seconds ---\n"]}],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","print(\"--- Starting Training (Simple RNN) ---\")\n","start_train_time = time.time()\n","model.train()\n","\n","for epoch in range(EPOCHS):\n","    epoch_start_time = time.time()\n","    epoch_loss = 0\n","    # Initialize hidden state for the start of the epoch (will be detached per batch)\n","    h = model.init_hidden(BATCH_SIZE)\n","\n","    for i, (inputs, targets) in enumerate(dataloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Detach hidden state from previous batch history\n","        h = h.detach()\n","\n","        optimizer.zero_grad()\n","        outputs, h = model(inputs, h)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        # Clip gradients\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    avg_epoch_loss = epoch_loss / len(dataloader)\n","    epoch_end_time = time.time()\n","    print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_epoch_loss:.4f} | Time: {epoch_end_time - epoch_start_time:.2f}s\")\n","\n","end_train_time = time.time()\n","print(f\"--- Training Finished in {end_train_time - start_train_time:.2f} seconds ---\")"]},{"cell_type":"markdown","metadata":{"id":"CEN2doYJjR-L"},"source":["## 5. Generating Text (Sampling)\n","\n","This is where the \"magic\" happens (or doesn't, given our simple model!). We feed the model a starting character (or sequence) and ask it to predict the next character. We then take that prediction, feed it back in, and repeat the process.\n","\n","*   **Priming:** We first feed the `start_phrase` to the model to get the hidden state into a reasonable context.\n","*   **Sampling:** Instead of always picking the *most likely* next character (which leads to boring, repetitive text), we *sample* from the probability distribution output by the model. The `temperature` parameter controls the randomness: lower temperature makes it more conservative, higher temperature makes it more adventurous (and often nonsensical)."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JatJAV0fjR-M","executionInfo":{"status":"ok","timestamp":1746349765681,"user_tz":-330,"elapsed":110,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"af40982f-0ef3-4e8a-fe02-d34ec4029c29"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Generating text starting with: 'natural language' --- (Temp=0.8)\n","Priming...\n","Generating...\n","\n","Generated Text:\n","natural language onderstanding the contextual nuances of the language data. \n","the tocumockgence concerned with the interactions between computer coprect information and insights tonta language understanding the conten\n","\n","--- Generating text starting with: 'the computer' --- (Temp=0.6)\n","Priming...\n","Generating...\n","\n","Generated Text:\n","the computers to process and analyze large amounts of natural language within them. \n","the technology can then ance commonly used for such tasks.\n","\n","the goal istacd the language within them. bthe technology can then \n"]}],"source":["def generate_text(model, start_phrase, length, temperature=0.8):\n","    \"\"\"Generates text using the trained simple RNN model.\"\"\"\n","    model.eval()\n","    start_phrase = start_phrase.lower()\n","    generated_text = start_phrase\n","\n","    # Initialize hidden state for generation (batch size 1)\n","    h = model.init_hidden(1)\n","\n","    # Prime the model with the start_phrase\n","    print(\"Priming...\")\n","    for char in start_phrase[:-1]:\n","        try:\n","            char_idx = char_to_int[char]\n","        except KeyError:\n","            continue # Skip chars not in vocab\n","        input_tensor = torch.tensor([[char_idx]], dtype=torch.long).to(device)\n","        with torch.no_grad():\n","            _, h = model(input_tensor, h)\n","\n","    # Set the first input for generation to the last char of the phrase\n","    try:\n","        last_char_idx = char_to_int[start_phrase[-1]]\n","        current_input = torch.tensor([[last_char_idx]], dtype=torch.long).to(device)\n","    except KeyError:\n","        print(f\"Error: Last character '{start_phrase[-1]}' not in vocabulary.\")\n","        return start_phrase\n","\n","    print(\"Generating...\")\n","    # Generation loop\n","    for _ in range(length):\n","        with torch.no_grad():\n","            output, h = model(current_input, h)\n","\n","            # Apply temperature\n","            output_dist = output.data.view(-1).div(temperature).exp()\n","            # Sample next character index\n","            top_char_idx = torch.multinomial(output_dist, 1)[0]\n","\n","            # Convert index to character\n","            predicted_char = int_to_char[top_char_idx.item()]\n","            generated_text += predicted_char\n","\n","            # Update input for next step\n","            current_input = torch.tensor([[top_char_idx.item()]], dtype=torch.long).to(device)\n","\n","    return generated_text\n","\n","# --- Example Generation ---\n","start_phrase = \"natural language\"\n","print(f\"\\n--- Generating text starting with: '{start_phrase}' --- (Temp=0.8)\")\n","generated_output = generate_text(model, start_phrase, length=GENERATION_LENGTH, temperature=0.8)\n","print(\"\\nGenerated Text:\")\n","print(generated_output)\n","\n","start_phrase_2 = \"the computer\"\n","print(f\"\\n--- Generating text starting with: '{start_phrase_2}' --- (Temp=0.6)\")\n","generated_output_2 = generate_text(model, start_phrase_2, length=GENERATION_LENGTH, temperature=0.6)\n","print(\"\\nGenerated Text:\")\n","print(generated_output_2)"]},{"cell_type":"markdown","metadata":{"id":"NFn-dj7bjR-M"},"source":["## 6. Conclusion & Karpathy Context\n","\n","We successfully built and trained a simple character-level RNN. As expected, the generated text likely shows some basic structure (e.g., forming word-like units, using spaces) but quickly devolves into repetition or nonsense.\n","\n","This aligns with Karpathy's observations:\n","*   **Effectiveness:** Even this simple model *learns something* about language structure from raw text, which is remarkable.\n","*   **Limitations:** Simple `nn.RNN` units suffer from vanishing gradients and cannot easily capture long-range dependencies. This is why the generated text struggles with coherence over longer stretches.\n","*   **Next Steps (as per Karpathy):** To get truly \"unreasonably effective\" results, one would typically use more advanced units like LSTMs or GRUs, larger datasets, and more extensive training, as these gated units are specifically designed to mitigate the vanishing gradient problem and better manage the hidden state (memory)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}