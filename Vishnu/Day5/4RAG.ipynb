{"cells":[{"cell_type":"markdown","metadata":{"id":"SH5UhiqMcNV_"},"source":["## Advanced RAG Demo: Conceptual Graph RAG\n","\n","This notebook provides a *conceptual demonstration* of how one might begin to incorporate graph-based elements into a RAG system. A full Graph RAG system often involves dedicated graph databases (like Neo4j) and more complex graph construction and traversal algorithms. Here, we'll simplify by:\n","\n","1.  Loading and chunking a PDF as usual.\n","2.  Using an LLM (Groq) to perform a basic form of **entity and simple relationship extraction** from text chunks. This is a key step in building a knowledge graph.\n","3.  Storing the original text chunks in a standard vector store (FAISS).\n","4.  For a given query, we will:\n","    a.  Retrieve relevant text chunks using vector search.\n","    b.  For these top chunks, use our (simulated) extracted entity/relationship information.\n","    c.  Augment the prompt to the final generator LLM with both the raw text of the retrieved chunks AND the extracted structured (graph-like) information.\n","\n","**Goal:** To show how adding structured relational information, even if simply extracted, could potentially help the LLM understand context better and answer more nuanced questions.\n","\n","**Note:** This is a simplified approach. True Graph RAG would involve building and querying an actual graph structure."]},{"cell_type":"markdown","metadata":{"id":"DJk3txaDcNWE"},"source":["### 1. Setup: Install Libraries and Import Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecDRoO-ZcNWF"},"outputs":[],"source":["!pip install -q langchain langchain-groq langchain-community pypdf faiss-cpu pypdf sentence-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDH9omlpcNWH"},"outputs":[],"source":["import os\n","import getpass\n","import json # For handling LLM output that might be structured\n","from langchain_groq import ChatGroq\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain.prompts import PromptTemplate\n","from langchain.chains.llm import LLMChain\n","from langchain_core.documents import Document # To handle chunk metadata"]},{"cell_type":"markdown","metadata":{"id":"FtDaxP4OcNWI"},"source":["### 2. Configure Groq API Key"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yb8-pBBrcNWJ"},"outputs":[],"source":["os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")"]},{"cell_type":"code","source":["os.makedirs(\"pdfs\", exist_ok=True)\n","\n","# Step 3: Download the PDF using requests\n","import requests\n","\n","url = \"https://cs229.stanford.edu/main_notes.pdf\"\n","pdf_path = \"pdfs/main_notes.pdf\"\n","\n","response = requests.get(url)\n","with open(pdf_path, \"wb\") as f:\n","    f.write(response.content)\n","\n","print(f\"PDF downloaded to: {pdf_path}\")"],"metadata":{"id":"KhMMSgM0cnNp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BSjK10O0cNWK"},"source":["### 3. Prepare PDF Document & Vector Store (Abbreviated)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yBjgqIMcNWL"},"outputs":[],"source":["raw_chunks = [] # Will store Document objects\n","vector_store = None\n","\n","if os.path.exists(pdf_path):\n","    loader = PyPDFLoader(pdf_path)\n","    documents = loader.load()\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n","    raw_chunks = text_splitter.split_documents(documents)\n","    print(f\"Split into {len(raw_chunks)} chunks.\")\n","\n","    if raw_chunks:\n","        embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n","        print(\"Creating FAISS vector store...\")\n","        vector_store = FAISS.from_documents(raw_chunks, embeddings)\n","        print(\"FAISS vector store created.\")\n","else:\n","    print(\"PDF not found, skipping processing.\")"]},{"cell_type":"markdown","metadata":{"id":"DkBhDE_UcNWL"},"source":["### 4. LLM for Entity & Relationship Extraction (Simplified)\n","\n","We'll define a prompt to ask the LLM to extract key concepts (entities) and simple relationships between them from a given text chunk. We'll ask for output in a structured format (like JSON) for easier parsing, though LLMs can be finicky with strict JSON."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_Pfx52hcNWM"},"outputs":[],"source":["extraction_llm = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.0) # Low temp for more factual extraction\n","\n","extraction_prompt_template = \"\"\"\n","From the following text, extract up to 5 key machine learning concepts (as entities) and any simple relationships between them (e.g., 'Concept A is a type of Concept B', 'Concept X uses Concept Y').\n","Present the output as a JSON object with two keys: \"entities\" (a list of strings) and \"relationships\" (a list of strings describing the relationships).\n","If no clear concepts or relationships are found, return empty lists.\n","\n","Example:\n","Text: 'Support Vector Machines (SVMs) are a type of supervised learning algorithm. SVMs can use different kernels, like the RBF kernel, to find a hyperplane.'\n","Output:\n","```json\n","{\n","  \"entities\": [\"Support Vector Machines\", \"supervised learning algorithm\", \"kernels\", \"RBF kernel\", \"hyperplane\"],\n","  \"relationships\": [\"Support Vector Machines are a type of supervised learning algorithm\", \"SVMs can use kernels\", \"RBF kernel is a type of kernel\"]\n","}\n","```\n","\n","Text to process:\n","---\n","{text_chunk}\n","---\n","\n","Output (JSON):\n","\"\"\"\n","extraction_prompt = PromptTemplate.from_template(extraction_prompt_template)\n","extraction_chain = LLMChain(llm=extraction_llm, prompt=extraction_prompt)\n","\n","def extract_graph_elements(text_chunk_content):\n","    try:\n","        response = extraction_chain.invoke({\"text_chunk\": text_chunk_content})\n","        # Try to parse the JSON output, cleaning it up if necessary\n","        # LLMs sometimes add ```json ... ``` around the actual JSON\n","        content = response['text']\n","        if content.startswith(\"```json\"):\n","            content = content[7:]\n","        if content.endswith(\"```\"):\n","            content = content[:-3]\n","\n","        data = json.loads(content.strip())\n","        return data\n","    except Exception as e:\n","        # print(f\"Error parsing JSON from LLM: {e}\\nLLM Output: {response['text']}\")\n","        return {\"entities\": [], \"relationships\": []} # Default on error"]},{"cell_type":"markdown","metadata":{"id":"kBDdafu6cNWO"},"source":["### 5. Augment Chunks with Extracted Graph-like Data (Simulated)\n","\n","In a real system, you might store this in a graph DB or link it to your chunks. Here, we'll extract it on-the-fly for the top retrieved chunks for simplicity in the demo. For a larger scale, pre-processing and storing these extractions would be necessary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiK4NTD4cNWO"},"outputs":[],"source":["# Let's try extracting for a few sample chunks to see it work\n","if raw_chunks and len(raw_chunks) > 5:\n","    print(\"--- Example Extractions ---\")\n","    for i, chunk_doc in enumerate(raw_chunks[0:2]): # Process first 2 chunks as example\n","        print(f\"\\nChunk {i+1} (first 100 chars): {chunk_doc.page_content[:100]}...\")\n","        graph_data = extract_graph_elements(chunk_doc.page_content)\n","        print(f\"Extracted Entities: {graph_data.get('entities')}\")\n","        print(f\"Extracted Relationships: {graph_data.get('relationships')}\")\n","        # In a real system, you'd add this to the chunk's metadata\n","        # chunk_doc.metadata['graph_elements'] = graph_data\n","    print(\"-------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"JLFI8hQqcNWO"},"source":["### 6. RAG with Augmented Context (Text + Extracted Graph Elements)\n","\n","Now, when we retrieve chunks for a query, we'll also (conceptually) fetch their pre-extracted graph elements and include them in the prompt to the final generator LLM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZNRVC7ecNWP"},"outputs":[],"source":["generator_llm = ChatGroq(model_name=\"llama3-70b-8192\", temperature=0.2)\n","\n","graph_augmented_qa_template = \"\"\"\n","You are a helpful AI assistant answering questions based on the provided context.\n","The context consists of text passages and some extracted key concepts and relationships from those passages.\n","Use all this information to formulate a comprehensive and accurate answer.\n","If the information is not sufficient, say so.\n","\n","--- CONTEXT ---\n","{context_str}\n","--- END CONTEXT ---\n","\n","Question: {query_str}\n","Helpful Answer:\n","\"\"\"\n","graph_qa_prompt = PromptTemplate.from_template(graph_augmented_qa_template)\n","graph_qa_chain = LLMChain(llm=generator_llm, prompt=graph_qa_prompt)\n","\n","def answer_with_graph_context(query, vector_store_retriever, k_chunks=2):\n","    if not vector_store_retriever:\n","        return \"Vector store not available.\"\n","\n","    print(f\"Retrieving top {k_chunks} chunks for query: '{query}'\")\n","    retrieved_docs = vector_store_retriever.get_relevant_documents(query)\n","\n","    if not retrieved_docs:\n","        return \"No relevant documents found.\"\n","\n","    augmented_context_parts = []\n","    for i, doc in enumerate(retrieved_docs):\n","        text_part = f\"Passage {i+1}:\\n{doc.page_content}\"\n","        augmented_context_parts.append(text_part)\n","\n","        # Simulate fetching/using pre-extracted graph data for this chunk\n","        # For this demo, we'll extract it on the fly. In production, this would be pre-computed.\n","        print(f\"Extracting graph elements for retrieved chunk {i+1}...\")\n","        graph_elements = extract_graph_elements(doc.page_content)\n","        if graph_elements[\"entities\"] or graph_elements[\"relationships\"]:\n","            graph_part = f\"Key Concepts/Relationships from Passage {i+1}:\\n\"\n","            if graph_elements[\"entities\"]:\n","                graph_part += f\"  Entities: {', '.join(graph_elements['entities'])}\\n\"\n","            if graph_elements[\"relationships\"]:\n","                graph_part += f\"  Relationships: {'; '.join(graph_elements['relationships'])}\"\n","            augmented_context_parts.append(graph_part)\n","\n","    full_context = \"\\n\\n---\\n\\n\".join(augmented_context_parts)\n","    # print(f\"\\n--- Augmented Context for LLM ---\\n{full_context[:1500]}...\\n---------------------------------\")\n","\n","    print(\"\\nGenerating answer with augmented context...\")\n","    response = graph_qa_chain.invoke({\"context_str\": full_context, \"query_str\": query})\n","    return response['text']"]},{"cell_type":"markdown","metadata":{"id":"bCQeq6gZcNWP"},"source":["### 7. Ask a Question using Conceptual Graph-Augmented RAG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9x7Zr-WPcNWQ"},"outputs":[],"source":["if vector_store:\n","    retriever = vector_store.as_retriever(search_kwargs={'k': 2}) # Retrieve top 2 chunks\n","\n","    # A query where relationships between concepts might be useful\n","    graph_query = \"Explain how Gaussian Discriminant Analysis relates to logistic regression, and what assumptions GDA makes.\"\n","    print(f\"\\n--- Conceptual Graph RAG Query: {graph_query} ---\")\n","    graph_answer = answer_with_graph_context(graph_query, retriever)\n","    print(f\"\\nConceptual Graph RAG Answer:\\n{graph_answer}\")\n","\n","    # Another example\n","    graph_query_2 = \"What are some types of kernels used in Support Vector Machines and how do they work?\"\n","    print(f\"\\n--- Conceptual Graph RAG Query: {graph_query_2} ---\")\n","    graph_answer_2 = answer_with_graph_context(graph_query_2, retriever, k_chunks=3)\n","    print(f\"\\nConceptual Graph RAG Answer:\\n{graph_answer_2}\")\n","else:\n","    print(\"Cannot run conceptual graph RAG as vector store is not available.\")"]},{"cell_type":"markdown","metadata":{"id":"jL7j2AOzcNWQ"},"source":["### 8. Conclusion\n","\n","This notebook offered a conceptual glimpse into Graph RAG. We simulated the extraction of entities and relationships from text chunks and showed how this structured information could be combined with raw text to form a richer context for the generator LLM.\n","\n","**Key Takeaways:**\n","- Extracting structured data (like graph elements) from unstructured text can add significant value.\n","- Providing both textual context and related structured information can potentially lead to more nuanced and accurate LLM responses, especially for questions involving relationships between concepts.\n","- Full Graph RAG systems are more complex, involving robust graph construction, storage (e.g., in a graph database like Neo4j, Kuzu, or NebulaGraph), and sophisticated graph traversal/querying techniques integrated into the retrieval process.\n","\n","This simplified demo highlights the potential direction. For a production system, you would invest in more robust entity/relationship extraction, potentially use a dedicated graph database, and design more sophisticated ways to combine graph query results with text retrieval."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}