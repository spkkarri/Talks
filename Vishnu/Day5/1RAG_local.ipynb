{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# simple_rag.ipynb\n","``````markdown\n","# Simple Retrieval-Augmented Generation (RAG) with Langchain\n","\n","This notebook demonstrates a basic RAG pipeline using Langchain.\n","We will:\n","1. Install necessary libraries.\n","2. Download a sample PDF document.\n","3. Load the PDF.\n","4. Split the document into manageable chunks.\n","5. Generate embeddings for the chunks using a Hugging Face model.\n","6. Store the chunks and their embeddings in a FAISS vector store.\n","7. Set up a Hugging Face LLM for generation.\n","8. Create a `RetrievalQA` chain to perform RAG.\n","9. Ask a question and get an answer based on the document."],"metadata":{"id":"intro_markdown_lc"},"id":"intro_markdown_lc"},{"cell_type":"code","source":["# @title 1. Install Dependencies\n","!pip install -q langchain langchain_community langchain_huggingface pypdf faiss-cpu sentence-transformers torch accelerate bitsandbytes\n","print(\"Dependencies installed.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Dependencies installed.\n"]}],"execution_count":null,"metadata":{"id":"install_deps_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619602808,"user_tz":-330,"elapsed":8999,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"f3b1addf-efa2-4537-98fa-765918d36587"},"id":"install_deps_lc"},{"cell_type":"code","source":["# @title 2. Setup and Download PDF\n","import os\n","import requests\n","\n","# Create a directory for PDFs if it doesn't exist\n","pdf_dir = \"pdfs\"\n","os.makedirs(pdf_dir, exist_ok=True)\n","\n","# URL of the PDF\n","pdf_url = \"https://cs229.stanford.edu/main_notes.pdf\"\n","pdf_filename = os.path.join(pdf_dir, \"cs229_main_notes.pdf\")\n","\n","# Download the PDF if it doesn't already exist\n","if not os.path.exists(pdf_filename):\n","    print(f\"Downloading {pdf_filename}...\")\n","    response = requests.get(pdf_url)\n","    with open(pdf_filename, \"wb\") as f:\n","        f.write(response.content)\n","    print(\"Download complete.\")\n","else:\n","    print(f\"{pdf_filename} already exists.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["pdfs/cs229_main_notes.pdf already exists.\n"]}],"execution_count":null,"metadata":{"id":"import_libs_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619603027,"user_tz":-330,"elapsed":206,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"b8d2809f-9a2f-4be2-fad8-c24e61855cd5"},"id":"import_libs_lc"},{"cell_type":"code","source":["# @title 3. Load the PDF Document\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","loader = PyPDFLoader(pdf_filename)\n","documents = loader.load()\n","\n","print(f\"Loaded {len(documents)} pages from the PDF.\")\n","# For demonstration, let's take only the first 50 pages to speed up processing\n","# Remove this line if you want to process the whole document (will take longer)\n","documents = documents[:100]\n","print(f\"Using {len(documents)} pages for this demo.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 227 pages from the PDF.\n","Using 100 pages for this demo.\n"]}],"execution_count":null,"metadata":{"id":"config_device_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619625208,"user_tz":-330,"elapsed":22180,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"bcff4eac-592c-4522-bb4d-766c733ed90a"},"id":"config_device_lc"},{"cell_type":"code","source":["# @title 4.  Split Documents\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n","doc_chunks = text_splitter.split_documents(documents)\n","\n","print(f\"Split the document into {len(doc_chunks)} chunks.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Split the document into 223 chunks.\n"]}],"execution_count":null,"metadata":{"id":"load_split_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619625252,"user_tz":-330,"elapsed":45,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"6556513b-3765-4342-e664-d72388f82704"},"id":"load_split_lc"},{"cell_type":"code","source":["# @title 5. Generate Embeddings\n","from langchain_huggingface import HuggingFaceEmbeddings\n","\n","# Using a smaller, efficient model for embeddings\n","# For potentially better results (and larger model size), consider \"BAAI/bge-small-en-v1.5\"\n","embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n","\n","print(\"Embedding model loaded.\")"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Embedding model loaded.\n"]}],"execution_count":null,"metadata":{"id":"config_embedding_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619645741,"user_tz":-330,"elapsed":20486,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"278adcf7-b8b3-4a2b-b767-fc2f3c01cb71"},"id":"config_embedding_lc"},{"cell_type":"code","source":["# @title 6. Create FAISS Vector Store (Local)\n","from langchain_community.vectorstores import FAISS\n","\n","vector_store = FAISS.from_documents(doc_chunks, embeddings)\n","print(\"Vector store created.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Vector store created.\n"]}],"execution_count":null,"metadata":{"id":"create_faiss_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619647088,"user_tz":-330,"elapsed":1345,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"05534bf4-7e3d-41b8-b20d-6046928abe8c"},"id":"create_faiss_lc"},{"cell_type":"code","source":["# @title 7. Configure LLM (Local Hugging Face Pipeline)\n","from langchain_huggingface import HuggingFacePipeline\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n","import torch\n","\n","# Define the model name\n","llm_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n","\n","pipe = pipeline(\"text2text-generation\", model=model,\n","    tokenizer=tokenizer,\n","    max_length=512,  # Max length of the generated text\n","    temperature=0.7, # Controls randomness: lower is more deterministic\n","    top_p=0.95,      # Nucleus sampling: considers the smallest set of tokens whose cumulative probability exceeds top_p\n","    repetition_penalty=1.2 # Penalizes repeated tokens\n",")\n","\n","\"\"\"\n","llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # MODIFIED\n","tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n","\n","pipe = pipeline(\n","    \"text-generation\", # MODIFIED\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=256,  # Max NEW tokens to generate (max_length includes prompt) # MODIFIED\n","    temperature=0.7,\n","    top_p=0.95,\n","    repetition_penalty=1.15 # Adjusted slightly\n",")\n","\"\"\"\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","print(\"LLM loaded and pipeline created.\")"],"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["LLM loaded and pipeline created.\n"]}],"execution_count":null,"metadata":{"id":"config_llm_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619648457,"user_tz":-330,"elapsed":1359,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"a81d482f-b1dc-4a0c-9f1e-8c983b706791"},"id":"config_llm_lc"},{"cell_type":"code","source":["# @title 8. Create RAG Chain using LCEL\n","\n","from langchain.chains import RetrievalQA\n","\n","# Create a retriever from the vector store\n","retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}) # Retrieve top 3 relevant chunks\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\", # \"stuff\" puts all retrieved text directly into the prompt\n","    retriever=retriever,\n","    return_source_documents=True # Optionally return source documents\n",")\n","\n","print(\"RetrievalQA chain created.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["RetrievalQA chain created.\n"]}],"execution_count":null,"metadata":{"id":"create_chain_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619649247,"user_tz":-330,"elapsed":789,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"7fc93ad8-d68d-449d-8e8b-7985effcc224"},"id":"create_chain_lc"},{"cell_type":"code","source":["query = \"What is  machine learning? \"\n","print(f\"Query: {query}\")\n","\n","# It's good practice to wrap LLM calls in a try-except block\n","\n","result = qa_chain.invoke({\"query\": query})\n","print(\"\\nAnswer:\")\n","print(result[\"result\"])\n","\n","#print(\"\\nSource Documents (first 100 chars of each):\")\n","#for i, doc in enumerate(result[\"source_documents\"]):\n","#    print(f\"Doc {i+1}: {doc.page_content[:200]}...\")\n"],"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Query: What is  machine learning? \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Answer:\n","predicting the output\n"]}],"execution_count":null,"metadata":{"id":"ask_question_lc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746619649658,"user_tz":-330,"elapsed":409,"user":{"displayName":"Dr. SRI PHANI","userId":"17121511473050189039"}},"outputId":"ec54594e-1c8c-41d4-fdaf-e870aa8c5b70"},"id":"ask_question_lc"}]}