{"cells":[{"cell_type":"markdown","metadata":{"id":"0sYH4D8DcHjb"},"source":["## Advanced RAG Demo: Agentic RAG with Langchain Agents\n","\n","This notebook explores a more advanced RAG setup using Langchain Agents. An agent can use a set of tools (like our PDF retriever) to answer complex questions. This mimics the ReAct (Reason + Act) paradigm where the LLM can reason about what information it needs, act to retrieve it, and then use that information to form an answer.\n","\n","We will:\n","1. Set up the PDF vector store as before.\n","2. Create a `Tool` from our retriever.\n","3. Initialize a Langchain Agent (e.g., a ReAct agent or a conversational agent that can use tools) with the Groq LLM.\n","4. Pose complex questions that might require the agent to decide to use the retrieval tool.\n","5. Observe the agent's thought process (intermediate steps)."]},{"cell_type":"markdown","metadata":{"id":"oOg0wgUZcHjg"},"source":["### 1. Setup: Install Libraries and Import Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZ94i2DwcHjh"},"outputs":[],"source":["# We might need langchain_experimental for some agent setups, or specific agent toolkits\n","!pip install -q langchain langchain-groq langchain-community pypdf faiss-cpu pypdf sentence-transformers langchain_experimental"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LM-yYBH8cHjj"},"outputs":[],"source":["import os\n","import getpass\n","from langchain_groq import ChatGroq\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain.chains import RetrievalQA\n","\n","# Agent-specific imports\n","from langchain.agents import Tool, AgentExecutor\n","from langchain.agents.react.agent import create_react_agent # A common ReAct agent\n","# Or, for a more modern approach with LCEL:\n","from langchain import hub\n","from langchain.agents import create_tool_calling_agent # if Groq supports tool calling well\n","                                                  # otherwise, ReAct is more robust for general LLMs\n","\n","from langchain.prompts import PromptTemplate # For ReAct agent prompt"]},{"cell_type":"markdown","metadata":{"id":"FL4yeKUecHjk"},"source":["### 2. Configure Groq API Key"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlZbd6r-cHjl"},"outputs":[],"source":["os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")"]},{"cell_type":"code","source":["os.makedirs(\"pdfs\", exist_ok=True)\n","\n","# Step 3: Download the PDF using requests\n","import requests\n","\n","url = \"https://cs229.stanford.edu/main_notes.pdf\"\n","pdf_path = \"pdfs/main_notes.pdf\"\n","\n","response = requests.get(url)\n","with open(pdf_path, \"wb\") as f:\n","    f.write(response.content)\n","\n","print(f\"PDF downloaded to: {pdf_path}\")"],"metadata":{"id":"p4VSN064cejJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYJ5kfM7cHjl"},"source":["### 3. Prepare PDF Document & Vector Store (Abbreviated - assuming it's done)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DydPlcrcHjm"},"outputs":[],"source":["chunks = []\n","vector_store = None\n","\n","if os.path.exists(pdf_path):\n","    loader = PyPDFLoader(pdf_path)\n","    documents = loader.load()\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n","    chunks = text_splitter.split_documents(documents)\n","    print(f\"Split into {len(chunks)} chunks.\")\n","\n","    if chunks:\n","        embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n","        print(\"Creating FAISS vector store...\")\n","        # For speed in demo, let's try to load if exists, else create\n","        if os.path.exists(\"faiss_index_cs229_agent\"):\n","             vector_store = FAISS.load_local(\"faiss_index_cs229_agent\", embeddings, allow_dangerous_deserialization=True)\n","             print(\"Loaded FAISS index from disk.\")\n","        else:\n","            vector_store = FAISS.from_documents(chunks, embeddings)\n","            vector_store.save_local(\"faiss_index_cs229_agent\")\n","            print(\"FAISS vector store created and saved.\")\n","else:\n","    print(\"PDF not found, agent will not have PDF tool.\")"]},{"cell_type":"markdown","metadata":{"id":"xvDq3jSZcHjn"},"source":["### 4. Initialize LLM and Create Retriever Tool"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WNHede0cHjo"},"outputs":[],"source":["llm = ChatGroq(model_name=\"llama3-70b-8192\", temperature=0.1) # Using a more capable model for agent tasks\n","\n","tools = []\n","if vector_store:\n","    # Create a RetrievalQA chain to be used as a tool\n","    pdf_qa_chain = RetrievalQA.from_chain_type(\n","        llm,\n","        retriever=vector_store.as_retriever(search_kwargs={'k': 3}),\n","        return_source_documents=True\n","    )\n","\n","    # Define the tool\n","    pdf_retrieval_tool = Tool(\n","        name=\"Stanford_CS229_Notes_Retriever\",\n","        func=lambda q: pdf_qa_chain.invoke({\"query\": q})['result'], # Agent expects string output from tool\n","        description=\"Useful for answering questions about machine learning concepts, algorithms, and theory based on the Stanford CS229 course notes. Input should be a specific question.\"\n","    )\n","    tools.append(pdf_retrieval_tool)\n","    print(\"PDF Retrieval tool created.\")\n","else:\n","    print(\"PDF Retrieval tool not created as vector store is unavailable.\")"]},{"cell_type":"markdown","metadata":{"id":"hoerlUYEcHjo"},"source":["### 5. Initialize the ReAct Agent\n","\n","We'll use the `create_react_agent` constructor. This requires a prompt that includes placeholders for `tools`, `tool_names`, `input`, `agent_scratchpad`, and `chat_history` (optional)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f_K-s5pcHjp"},"outputs":[],"source":["agent_executor = None\n","if tools:\n","    # Get the ReAct prompt from Langchain Hub\n","    # This prompt is engineered to guide the LLM in the ReAct cycle (Thought, Action, Observation)\n","    prompt = hub.pull(\"hwchase17/react\")\n","    # print(prompt.template) # You can inspect the prompt template\n","\n","    # Construct the ReAct agent\n","    agent = create_react_agent(llm, tools, prompt)\n","\n","    # Create an agent executor\n","    agent_executor = AgentExecutor(\n","        agent=agent,\n","        tools=tools,\n","        verbose=True, # Shows the agent's thought process\n","        handle_parsing_errors=True, # Important for robustness\n","        max_iterations=5 # Prevent runaway agents\n","    )\n","    print(\"ReAct agent executor created.\")\n","else:\n","    print(\"Agent not created as no tools are available.\")"]},{"cell_type":"markdown","metadata":{"id":"PGDOEWXNcHjp"},"source":["### 6. Run the Agent with Complex Questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24pOcpOScHjq"},"outputs":[],"source":["if agent_executor:\n","    query1 = \"What are the key differences between Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) as described in the CS229 notes?\"\n","    print(f\"\\n--- Agent Query 1: {query1} ---\")\n","    try:\n","        response1 = agent_executor.invoke({\"input\": query1})\n","        print(f\"\\nAgent Response 1: {response1['output']}\")\n","    except Exception as e:\n","        print(f\"Error during agent execution: {e}\")\n","\n","    query2 = \"Could you explain the bias-variance tradeoff and then tell me how it relates to model selection, using information from the CS229 notes?\"\n","    print(f\"\\n--- Agent Query 2: {query2} ---\")\n","    try:\n","        response2 = agent_executor.invoke({\"input\": query2})\n","        print(f\"\\nAgent Response 2: {response2['output']}\")\n","    except Exception as e:\n","        print(f\"Error during agent execution: {e}\")\n","\n","    # A question the agent should realize it cannot answer from the PDF tool\n","    query3 = \"What is the current weather in Palo Alto?\"\n","    print(f\"\\n--- Agent Query 3: {query3} ---\")\n","    try:\n","        response3 = agent_executor.invoke({\"input\": query3})\n","        print(f\"\\nAgent Response 3: {response3['output']}\")\n","    except Exception as e:\n","        print(f\"Error during agent execution: {e}\")\n","else:\n","    print(\"Cannot run agent queries as agent_executor is not available.\")"]},{"cell_type":"markdown","metadata":{"id":"rBkoLjm6cHjq"},"source":["### 7. Conclusion\n","\n","This notebook demonstrated an agentic RAG system. The agent, powered by a Groq LLM, could reason about when to use its PDF retrieval tool to answer questions. The `verbose=True` setting allowed us to see the 'Thought:', 'Action:', 'Observation:' cycle, which is characteristic of ReAct agents. This approach enables handling more complex, multi-step queries where the LLM needs to actively seek information before formulating a final answer. For production, you'd want more sophisticated error handling and potentially more tools for the agent."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}